/**
 * OpenAI Integration for CouncilPAD
 * 
 * Handles communication with OpenAI API using assembled prompts
 */

import OpenAI from 'openai';
import type { AssembledPrompt, CouncilResponse } from '../types/council';

const apiKey = process.env.OPENAI_API_KEY || 'placeholder-key';

if (!process.env.OPENAI_API_KEY) {
  console.warn('OPENAI_API_KEY not set - OpenAI integration will not work');
}

const openai = new OpenAI({
  apiKey,
  // Don't validate key during build
  dangerouslyAllowBrowser: false,
});

// ============================================================
// COUNCIL QUERY
// ============================================================

export interface CouncilQueryOptions {
  model?: string;
  temperature?: number;
  max_tokens?: number;
}

export async function queryCouncil(
  question: string,
  assembledPrompt: AssembledPrompt,
  options: CouncilQueryOptions = {}
): Promise<{ response: string; tokens_used: number }> {
  const {
    model = 'gpt-4-turbo-preview',
    temperature = 0.7,
    max_tokens = 2000,
  } = options;

  try {
    const completion = await openai.chat.completions.create({
      model,
      temperature,
      max_tokens,
      messages: [
        {
          role: 'system',
          content: assembledPrompt.system_prompt,
        },
        {
          role: 'user',
          content: question,
        },
      ],
    });

    const response = completion.choices[0]?.message?.content || '';
    const tokens_used = completion.usage?.total_tokens || 0;

    return {
      response,
      tokens_used,
    };
  } catch (error) {
    console.error('Error querying OpenAI:', error);
    throw error;
  }
}

// ============================================================
// STREAMING COUNCIL QUERY
// ============================================================

export async function queryCouncilStream(
  question: string,
  assembledPrompt: AssembledPrompt,
  onChunk: (chunk: string) => void,
  options: CouncilQueryOptions = {}
): Promise<{ tokens_used: number }> {
  const {
    model = 'gpt-4-turbo-preview',
    temperature = 0.7,
    max_tokens = 2000,
  } = options;

  try {
    const stream = await openai.chat.completions.create({
      model,
      temperature,
      max_tokens,
      messages: [
        {
          role: 'system',
          content: assembledPrompt.system_prompt,
        },
        {
          role: 'user',
          content: question,
        },
      ],
      stream: true,
    });

    let tokens_used = 0;

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        onChunk(content);
      }
      
      // Note: streaming doesn't provide accurate token counts in real-time
      // We'll estimate based on content length
      if (chunk.choices[0]?.finish_reason === 'stop') {
        // Rough estimation: ~4 chars per token
        tokens_used = Math.ceil(assembledPrompt.system_prompt.length / 4);
      }
    }

    return { tokens_used };
  } catch (error) {
    console.error('Error querying OpenAI (stream):', error);
    throw error;
  }
}

// ============================================================
// MOCK RESPONSE FOR DEVELOPMENT
// ============================================================

export async function mockCouncilQuery(
  question: string,
  assembledPrompt: AssembledPrompt
): Promise<{ response: string; tokens_used: number }> {
  // Simulate API delay
  await new Promise(resolve => setTimeout(resolve, 1000));

  const activeMembers = assembledPrompt.active_members
    .map(m => m.statue_name || m.nfc_tag_id)
    .join(', ');

  const response = `
## Council Response (Mock)

**Active Members**: ${activeMembers}
**Preset**: ${assembledPrompt.preset}

---

**Question**: ${question}

This is a mock response. In production, this would be generated by GPT-4 using the assembled system prompt that integrates ${assembledPrompt.active_fragments.length} prompt fragments from the active council members.

The system prompt includes:
- ${assembledPrompt.active_fragments.filter(f => f.category === 'axiom').length} axioms
- ${assembledPrompt.active_fragments.filter(f => f.category === 'role').length} role perspectives
- ${assembledPrompt.active_fragments.filter(f => f.category === 'tone_modifier').length} tone modifiers
- ${assembledPrompt.active_fragments.filter(f => f.category === 'synthesis_strategy').length} synthesis strategy

The assembled prompt would guide GPT-4 to respond as a composite intelligence, preserving tensions between perspectives while offering integrated wisdom.

**Follow-up questions:**
1. What are we not seeing by framing it this way?
2. How might this look from a non-human perspective?
3. What needs to shift at a deeper level?
  `.trim();

  return {
    response,
    tokens_used: 150, // Mock token count
  };
}

